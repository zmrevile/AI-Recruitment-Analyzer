#!/usr/bin/env python3
"""
æœ¬åœ°EmbeddingæœåŠ¡
ä½¿ç”¨sentence-transformersæä¾›æ–‡æœ¬å‘é‡åŒ–åŠŸèƒ½
æ”¯æŒç¼“å­˜ã€æ‰¹å¤„ç†ã€GPUåŠ é€Ÿç­‰ä¼˜åŒ–åŠŸèƒ½
"""

import numpy as np
import hashlib
import time
import torch
from typing import List, Optional, Dict, Union
from functools import lru_cache
from threading import Lock
from app.utils.logger import spark_embedding_logger
from app.config.settings import settings


class LocalEmbeddings:
    """æœ¬åœ°EmbeddingæœåŠ¡ - ä¼˜åŒ–ç‰ˆ"""
    
    _instance = None
    _lock = Lock()
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(LocalEmbeddings, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–æœ¬åœ°EmbeddingæœåŠ¡"""
        if hasattr(self, '_initialized'):
            return
            
        self.local_model = None
        self.device = self._get_optimal_device()
        self.model_name = getattr(settings, 'EMBEDDING_MODEL', 'all-MiniLM-L6-v2')
        self.batch_size = getattr(settings, 'EMBEDDING_BATCH_SIZE', 32)
        self.cache_size = getattr(settings, 'EMBEDDING_CACHE_SIZE', 1000)
        self.dimension = 384  # é»˜è®¤ç»´åº¦
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'total_time': 0.0,
            'avg_time': 0.0
        }
        
        self._text_cache: Dict[str, List[float]] = {}
        self._cache_lock = Lock()
        
        self._init_local_model()
        self._initialized = True
    
    def _get_optimal_device(self) -> str:
        """è·å–æœ€ä¼˜è®¾å¤‡"""
        if torch.cuda.is_available():
            device = 'cuda'
            spark_embedding_logger.info(f"ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿ: {torch.cuda.get_device_name()}")
        else:
            device = 'cpu'
            spark_embedding_logger.info("ğŸ’» ä½¿ç”¨CPUè®¡ç®—")
        return device
    
    def _init_local_model(self):
        """åˆå§‹åŒ–æœ¬åœ°embeddingæ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer
            
            spark_embedding_logger.info(f"ğŸ”„ åŠ è½½embeddingæ¨¡å‹: {self.model_name}")
            self.local_model = SentenceTransformer(self.model_name, device=self.device)
            
            # è·å–å®é™…å‘é‡ç»´åº¦
            self.dimension = self.local_model.get_sentence_embedding_dimension()
            
            spark_embedding_logger.info(f"âœ… æœ¬åœ°embeddingæ¨¡å‹åŠ è½½æˆåŠŸ (ç»´åº¦: {self.dimension}, è®¾å¤‡: {self.device})")
            
        except ImportError:
            spark_embedding_logger.error("âŒ sentence-transformersæœªå®‰è£…ï¼Œè¯·å®‰è£…ï¼špip install sentence-transformers")
            raise
        except Exception as e:
            spark_embedding_logger.error(f"âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _get_text_hash(self, text: str) -> str:
        """ç”Ÿæˆæ–‡æœ¬å“ˆå¸Œç”¨äºç¼“å­˜"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _get_from_cache(self, text: str) -> Optional[List[float]]:
        """ä»ç¼“å­˜è·å–embedding"""
        text_hash = self._get_text_hash(text)
        with self._cache_lock:
            return self._text_cache.get(text_hash)
    
    def _save_to_cache(self, text: str, embedding: List[float]):
        """ä¿å­˜embeddingåˆ°ç¼“å­˜"""
        text_hash = self._get_text_hash(text)
        with self._cache_lock:
            # ç®€å•çš„LRUå®ç°
            if len(self._text_cache) >= self.cache_size:
                # åˆ é™¤æœ€æ—§çš„ä¸€ä¸ª
                oldest_key = next(iter(self._text_cache))
                del self._text_cache[oldest_key]
            
            self._text_cache[text_hash] = embedding
    
    def _preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if not text or not text.strip():
            return ""
        
        # åŸºæœ¬æ¸…ç†
        text = text.strip()
        # ç§»é™¤è¿‡å¤šçš„ç©ºç™½å­—ç¬¦
        text = ' '.join(text.split())
        
        return text
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£ - æ‰¹å¤„ç†ä¼˜åŒ–ç‰ˆ"""
        start_time = time.time()
        
        if not texts:
            return []
        
        # é¢„å¤„ç†æ–‡æœ¬
        processed_texts = [self._preprocess_text(text) for text in texts]
        
        # æ£€æŸ¥ç¼“å­˜
        results = []
        uncached_texts = []
        uncached_indices = []
        
        for i, text in enumerate(processed_texts):
            cached_embedding = self._get_from_cache(text)
            if cached_embedding is not None:
                results.append((i, cached_embedding))
                self.stats['cache_hits'] += 1
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # å¯¹æœªç¼“å­˜çš„æ–‡æœ¬è¿›è¡Œæ‰¹å¤„ç†
        if uncached_texts:
            uncached_embeddings = self._embed_with_local_model(uncached_texts)
            
            # ä¿å­˜åˆ°ç¼“å­˜å¹¶æ·»åŠ åˆ°ç»“æœ
            for idx, (original_idx, embedding) in enumerate(zip(uncached_indices, uncached_embeddings)):
                self._save_to_cache(processed_texts[original_idx], embedding)
                results.append((original_idx, embedding))
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        results.sort(key=lambda x: x[0])
        final_results = [embedding for _, embedding in results]
        
        # æ›´æ–°ç»Ÿè®¡
        elapsed_time = time.time() - start_time
        self._update_stats(len(texts), elapsed_time)
        
        return final_results
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥æŸ¥è¯¢"""
        return self.embed_documents([text])[0]
    
    def _embed_with_local_model(self, texts: List[str]) -> List[List[float]]:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆembedding - æ‰¹å¤„ç†ç‰ˆ"""
        if not texts:
            return []
            
        if self.local_model is not None:
            try:
                # åˆ†æ‰¹å¤„ç†å¤§é‡æ–‡æœ¬
                all_embeddings = []
                
                for i in range(0, len(texts), self.batch_size):
                    batch_texts = texts[i:i + self.batch_size]
                    batch_embeddings = self.local_model.encode(
                        batch_texts,
                        convert_to_tensor=False,
                        show_progress_bar=False,
                        batch_size=min(self.batch_size, len(batch_texts))
                    )
                    all_embeddings.extend(batch_embeddings.tolist())
                
                return all_embeddings
                
            except Exception as e:
                spark_embedding_logger.error(f"âŒ embeddingç”Ÿæˆå¤±è´¥: {e}")
                return [self._generate_fallback_vector(text) for text in texts]
        else:
            spark_embedding_logger.error("âŒ æœ¬åœ°æ¨¡å‹æœªåˆå§‹åŒ–")
            return [self._generate_fallback_vector(text) for text in texts]
    
    def _generate_fallback_vector(self, text: str) -> List[float]:
        """ç”Ÿæˆå¤‡ç”¨å‘é‡ - æ”¹è¿›ç‰ˆ"""
        if not text:
            text = "empty_text"
            
        # ä½¿ç”¨æ›´ç¨³å®šçš„å“ˆå¸Œæ–¹æ³•
        text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()
        seed = int(text_hash[:8], 16) % (2**32)
        
        np.random.seed(seed)
        vector = np.random.randn(self.dimension)
        vector = vector / np.linalg.norm(vector)
        
        return vector.tolist()
    
    def _update_stats(self, num_texts: int, elapsed_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.stats['total_requests'] += num_texts
        self.stats['total_time'] += elapsed_time
        self.stats['avg_time'] = self.stats['total_time'] / max(self.stats['total_requests'], 1)
    
    def get_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        cache_hit_rate = (self.stats['cache_hits'] / max(self.stats['total_requests'], 1)) * 100
        
        return {
            **self.stats,
            'cache_hit_rate': f"{cache_hit_rate:.2f}%",
            'cache_size': len(self._text_cache),
            'model_name': self.model_name,
            'device': self.device,
            'dimension': self.dimension
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._cache_lock:
            self._text_cache.clear()
        spark_embedding_logger.info("ğŸ§¹ embeddingç¼“å­˜å·²æ¸…ç©º")
    
    def warmup(self):
        """æ¨¡å‹é¢„çƒ­"""
        if self.local_model:
            spark_embedding_logger.info("ğŸ”¥ å¼€å§‹æ¨¡å‹é¢„çƒ­...")
            dummy_texts = ["é¢„çƒ­æ–‡æœ¬", "warmup text", "æ¨¡å‹åˆå§‹åŒ–"]
            self.embed_documents(dummy_texts)
            spark_embedding_logger.info("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")


# å‘åå…¼å®¹ - é‡å‘½åä½†ä¿æŒæ¥å£ä¸€è‡´
SparkEmbeddings = LocalEmbeddings

# å…¨å±€å®ä¾‹
_global_embeddings = None

def get_embeddings() -> LocalEmbeddings:
    """è·å–embeddingå®ä¾‹ - å•ä¾‹æ¨¡å¼"""
    global _global_embeddings
    if _global_embeddings is None:
        _global_embeddings = LocalEmbeddings()
    return _global_embeddings


# ä¾¿æ·å‡½æ•°
def embed_text(text: str) -> List[float]:
    """å¿«é€ŸåµŒå…¥å•ä¸ªæ–‡æœ¬"""
    return get_embeddings().embed_query(text)

def embed_texts(texts: List[str]) -> List[List[float]]:
    """å¿«é€ŸåµŒå…¥å¤šä¸ªæ–‡æœ¬"""
    return get_embeddings().embed_documents(texts)

def get_embedding_stats() -> Dict:
    """è·å–embeddingæ€§èƒ½ç»Ÿè®¡"""
    return get_embeddings().get_stats()

def clear_embedding_cache():
    """æ¸…ç©ºembeddingç¼“å­˜"""
    get_embeddings().clear_cache() 