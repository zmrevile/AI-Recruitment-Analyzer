"""
å²—ä½éœ€æ±‚åˆ†æå™¨ - ä½¿ç”¨æ˜Ÿç«å¤§æ¨¡å‹åˆ†æå²—ä½è¦æ±‚å¹¶æ„å»ºå‘é‡æ•°æ®åº“
"""
import os
import chromadb
from langchain.text_splitter import RecursiveCharacterTextSplitter
from app.core.spark_llm import SparkLLM
from app.core.spark_embedding import SparkEmbeddings
from langchain_core.messages import HumanMessage
from langchain_community.vectorstores import Chroma
from typing import Dict, List
import json
import re
from app.utils.logger import job_analyzer_logger
from app.utils.json_helper import JSONHelper

class JobAnalyzer:
    def __init__(self, spark_config: Dict):
        self.spark_config = spark_config
        self.embeddings = SparkEmbeddings(
            app_id=spark_config.get("app_id", ""),
            api_key=spark_config.get("api_key", ""),
            api_secret=spark_config.get("api_secret", "")
        )
        self.llm = SparkLLM(
            app_id=spark_config.get("app_id", ""),
            api_key=spark_config.get("api_key", ""),
            api_secret=spark_config.get("api_secret", ""),
            temperature=0.3
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            length_function=len,
        )
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vector_store = None
        self.job_content = ""
        self.structured_job_data = {}
        
    def load_job_requirement(self, file_path: str) -> str:
        """ä»æ–‡ä»¶ä¸­åŠ è½½å²—ä½è¦æ±‚"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            job_analyzer_logger.error(f"å²—ä½è¦æ±‚æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return ""
    
    def extract_structured_job_info(self, job_text: str) -> Dict:
        """ä½¿ç”¨LLMæå–ç»“æ„åŒ–çš„å²—ä½ä¿¡æ¯"""
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹å²—ä½è¦æ±‚ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š

        å²—ä½è¦æ±‚æ–‡æœ¬ï¼š
        {job_text}

        è¯·æŒ‰ä»¥ä¸‹ç»“æ„æå–ä¿¡æ¯ï¼š
        {{
            "job_title": "èŒä½åç§°",
            "company": "å…¬å¸åç§°", 
            "department": "éƒ¨é—¨",
            "salary_range": "è–ªèµ„èŒƒå›´",
            "experience_requirement": {{
                "min_years": æœ€ä½å¹´é™æ•°å­—,
                "max_years": æœ€é«˜å¹´é™æ•°å­—,
                "level": "çº§åˆ«æè¿°"
            }},
            "education_requirement": {{
                "min_degree": "æœ€ä½å­¦å†è¦æ±‚",
                "preferred_majors": ["ä¸“ä¸š1", "ä¸“ä¸š2"]
            }},
            "hard_requirements": {{
                "å¿…é¡»æŠ€èƒ½": ["æŠ€èƒ½1", "æŠ€èƒ½2"],
                "å¿…é¡»ç»éªŒ": ["ç»éªŒ1", "ç»éªŒ2"]
            }},
            "core_skills": {{
                "æŠ€èƒ½åç§°": {{"weight": æƒé‡æ•°å­—, "level": "è¦æ±‚çº§åˆ«", "required": true/false}},
                "è®¡ç®—æœºè§†è§‰": {{"weight": 0.35, "level": "ç²¾é€š", "required": true}}
            }},
            "preferred_skills": {{
                "æŠ€èƒ½åç§°": {{"weight": æƒé‡æ•°å­—, "level": "è¦æ±‚çº§åˆ«"}},
                "CUDAç¼–ç¨‹": {{"weight": 0.1, "level": "äº†è§£"}}
            }},
            "project_requirements": ["é¡¹ç›®è¦æ±‚1", "é¡¹ç›®è¦æ±‚2"],
            "bonus_points": ["åŠ åˆ†é¡¹1", "åŠ åˆ†é¡¹2"]
        }}
        
        æ³¨æ„ï¼š
        1. æƒé‡æ•°å­—åº”ä¸º0-1ä¹‹é—´çš„å°æ•°ï¼Œæ‰€æœ‰æ ¸å¿ƒæŠ€èƒ½æƒé‡ä¹‹å’Œåº”æ¥è¿‘1
        2. å¦‚æœä¿¡æ¯ä¸å­˜åœ¨ï¼Œè¯·ç”¨"æœªæåŠ"æˆ–ç©ºæ•°ç»„è¡¨ç¤º
        3. ç¡®ä¿JSONæ ¼å¼æ­£ç¡®
        """
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        
        # æ‰“å°åŸå§‹å“åº”å†…å®¹ç”¨äºè°ƒè¯•
        job_analyzer_logger.debug(f"ğŸ” æ˜Ÿç«å¤§æ¨¡å‹åŸå§‹è¿”å›å†…å®¹:")
        job_analyzer_logger.debug(f"'{response.content}'")
        job_analyzer_logger.debug(f"ğŸ” å†…å®¹é•¿åº¦: {len(response.content)}")
        job_analyzer_logger.debug(f"ğŸ” å‰100ä¸ªå­—ç¬¦: {repr(response.content[:100])}")
        
        # ä½¿ç”¨ç»Ÿä¸€çš„JSONè§£ææ–¹æ³•
        structured_data = JSONHelper.parse_llm_response(response.content, "å²—ä½ä¿¡æ¯æå–")
        
        if structured_data is None:
            raise ValueError(f"å²—ä½ä¿¡æ¯æå–å¤±è´¥ï¼šæ— æ³•è§£æLLMè¿”å›çš„JSONæ ¼å¼")
            
        return structured_data
    

    

    
    def create_vector_database(self, job_text: str) -> bool:
        """åˆ›å»ºå²—ä½è¦æ±‚çš„å‘é‡æ•°æ®åº“"""
        try:
            # åˆ†å‰²æ–‡æœ¬
            chunks = self.text_splitter.split_text(job_text)
            
            # åˆ›å»ºChromaå‘é‡å­˜å‚¨
            persist_directory = "./chroma_db_job"
            self.vector_store = Chroma.from_texts(
                texts=chunks,
                embedding=self.embeddings,
                persist_directory=persist_directory,
                collection_name="job_requirements"
            )
            
            return True
        except Exception as e:
            job_analyzer_logger.error(f"å²—ä½è¦æ±‚å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥: {e}")
            return False
    
    def search_job_context(self, query: str, k: int = 3) -> List[str]:
        """æœç´¢å²—ä½è¦æ±‚ç›¸å…³å†…å®¹"""
        if not self.vector_store:
            return []
        
        try:
            docs = self.vector_store.similarity_search(query, k=k)
            return [doc.page_content for doc in docs]
        except Exception as e:
            job_analyzer_logger.error(f"å²—ä½è¦æ±‚æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def process_job_requirement(self, file_path: str) -> bool:
        """å¤„ç†å²—ä½è¦æ±‚æ–‡ä»¶"""
        try:
            # 1. åŠ è½½å²—ä½è¦æ±‚æ–‡ä»¶
            self.job_content = self.load_job_requirement(file_path)
            if not self.job_content:
                return False
            
            # 2. æå–ç»“æ„åŒ–ä¿¡æ¯
            self.structured_job_data = self.extract_structured_job_info(self.job_content)
            
            # 3. åˆ›å»ºå‘é‡æ•°æ®åº“
            success = self.create_vector_database(self.job_content)
            
            return success
        except Exception as e:
            job_analyzer_logger.error(f"å²—ä½è¦æ±‚å¤„ç†å¤±è´¥: {e}")
            return False
    
    def get_job_info(self) -> Dict:
        """è·å–å²—ä½åŸºæœ¬ä¿¡æ¯"""
        return self.structured_job_data
    
    def get_job_summary(self) -> str:
        """è·å–å²—ä½è¦æ±‚æ‘˜è¦"""
        if not self.structured_job_data:
            return "å²—ä½ä¿¡æ¯æœªåŠ è½½"
        
        job_title = self.structured_job_data.get("job_title", "æœªçŸ¥èŒä½")
        company = self.structured_job_data.get("company", "æœªçŸ¥å…¬å¸")
        exp_req = self.structured_job_data.get("experience_requirement", {})
        
        summary = f"èŒä½ï¼š{job_title}\n"
        summary += f"å…¬å¸ï¼š{company}\n"
        summary += f"ç»éªŒè¦æ±‚ï¼š{exp_req.get('min_years', 0)}-{exp_req.get('max_years', 0)}å¹´\n"
        
        # æ ¸å¿ƒæŠ€èƒ½æ¦‚è§ˆ
        core_skills = self.structured_job_data.get("core_skills", {})
        if core_skills:
            top_skills = sorted(core_skills.items(), 
                              key=lambda x: x[1].get("weight", 0), 
                              reverse=True)[:3]
            skills_str = ", ".join([skill for skill, _ in top_skills])
            summary += f"æ ¸å¿ƒæŠ€èƒ½ï¼š{skills_str}"
        
        return summary
    
